### PATTERN: RETRY ON FAILURE ###

## Rationale
A service may fail the first time it makes a request of another service, whether it is application service or cloud managed service. Those failures might be very short-lived but without any retrial policy in place, the requesting service is forced to go into failure handling mode. With a retrial policy in place, the underlying infrastructure can retry without the knowledge of the requesting service, thus providing improved failure handling.

## Solution
There are several retry strategies that can be applied:
*	Fixed interval: retrying at a fixed rate. One should be cautious with choosing very short intervals and a high number of retries, as this can have a cascading effect.
*	Exponential backoff: using progressively longer waits between retries. Basically the retry algorithm looks like the following:
      * Identify if the fault is a transient fault.
      *	Define the maximum retry count.
      *	Retry the service call and increment the retry count.
      *	If the call succeeds, return the result to the caller.
      *	If we are still getting the same fault, Increase the delay period for next retry.
      *	Keep retrying and keep increasing the delay period until the maximum retry count is hit.
      *	If the call is failing even after maximum retries, let the caller module know that the target service is unavailable.

There is significant library support for implementing exponential backoff such as Polly (.NET), Spring-retry (Spring), https://github.com/cenkalti/backoff (Go), https://github.com/litl/backoff (Python) and https://github.com/stripe/stripe-ruby (Ruby library for the Stripe Api)

For applications deployed in container clusters, using service mesh such as Istio or AWS App Mesh provides declarative retry capabilities. Advantage is that this puts more of the resilience implementation into the infrastructure so that applications can contain pure business logic.


